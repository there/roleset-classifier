{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguating PropBank Rolesets with Neural Network Classifiers\n",
    "The following code implements a TensorFlow model that distinguishes between [PropBank][1] rolesets for a given lemma. This task can essentially be treated as a word sense disambiguation problem, where the different rolesets correspond to different, coarse-grained word senses. This model is trained on sentences from the [Penn Treebank][2] and the [English Web Treebank][3], annotated under the [Universal Dependencies][4] framework.\n",
    "\n",
    "## Abstract\n",
    "Semantic role labeling (SRL) is a task where phrases are assigned labels describing their relationship to the predicate of a sentence. PropBank, a text corpus used in SRL, defines frames for predicates (e.g., verbs) in the Penn Treebank, a syntactically-annotated text corpus used in computational linguistics and natural language processing. In PropBank, a set of roles for a predicate is called a roleset, and a predicate might have multiple possible rolesets (i.e., the roleset for \"firing an employee\" vs. \"firing a missile\"). Disambiguating rolesets enables Army analysts to extract and filter relevant information from text sources. This work implements a Tensorflow 4-layer neural network classifier for each PropBank verb with more than one roleset. The classifiers are trained on sentences from the Penn Treebank and English Web Treebank, using features extracted from dependency-based syntactic n-grams. A synonym replacement strategy is used to augment the training data. The models use the Adam optimizer and are regularized with early stopping and 50% dropout for each hidden layer. The models are tuned on a development set and correctly classify 91.2% of ambiguous predicates in the development set and 90.1% of ambiguous predicates in the test set.\n",
    "\n",
    "## Table of contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Merging the PropBank and UD files](#Merging-the-Propbank-and-UD-files)\n",
    "3. [Preparing the data](#Preparing-the-data)\n",
    "4. [Building the classifier](#Building-the-classifier)\n",
    "5. [Results](#Results)\n",
    "\n",
    "## Setup\n",
    "### Installing TensorFlow\n",
    "Install and activate [the nightly build for TensorFlow in an Anaconda 3 environment][5].\n",
    "\n",
    "`$ conda create -n tensorflow pip python=3.6`\n",
    "\n",
    "`$ source activate tensorflow`\n",
    "\n",
    "`(tensorflow)$ pip install tf-nightly`\n",
    "\n",
    "### Other dependencies\n",
    "\n",
    "Install [NLTK][6], [TensorFlow Hub][7], and [pandas][8]. Due to [a bug in the current version of pandas][9], an older version of pandas is needed.\n",
    "\n",
    "`(tensorflow)$ pip install -U tensorflow_hub`\n",
    "\n",
    "`(tensorflow)$ pip install -U nltk`\n",
    "\n",
    "`(tensorflow)$ pip install pandas==0.22`\n",
    "\n",
    "[1]: https://propbank.github.io/\n",
    "[2]: https://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports\n",
    "[3]: https://catalog.ldc.upenn.edu/LDC2012T13\n",
    "[4]: http://universaldependencies.org/\n",
    "[5]: https://www.tensorflow.org/install/install_linux#use_pip_in_anaconda\n",
    "[6]: https://www.nltk.org/install.html\n",
    "[7]: https://www.tensorflow.org/hub/\n",
    "[8]: https://pandas.pydata.org/\n",
    "[9]: https://stackoverflow.com/a/50836510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tphan/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import propbank\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [WordNet from NLTK][1]. This only needs to be done the first time the program is run.\n",
    "\n",
    "[1]: http://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Propbank and UD files\n",
    "The following code merges Propbank roleset data with UD-annotated sentences from the English Web Treebank. These data are available on Github:\n",
    "\n",
    "- [propbank-release](https://github.com/propbank/propbank-release)\n",
    "- [UD_English-EWT](https://github.com/UniversalDependencies/UD_English-EWT)\n",
    "\n",
    "The UD-annotated Penn TreeBank is not publicly available, but this code can also be used on that data.\n",
    "\n",
    "[1]: https://github.com/propbank/propbank-release\n",
    "[2]: https://github.com/UniversalDependencies/UD_English-EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKEL_REGEX = re.compile(r'^\\S+\\s+\\d+\\s+(\\d+)\\s+\\[WORD\\]\\s+\\S+\\s+(?:\\(*[A-Z]*)*\\*\\)*\\s+\\S+\\s+(\\S+)\\s*([\\S\\s]*)$')\n",
    "REL_REGEX = re.compile(r'\\(V\\*\\)')\n",
    "ARG_REGEX = re.compile(r'^\\(([A-Z\\d-]+)\\*\\)*$')\n",
    "SPACE_REGEX = re.compile(r'\\s+')\n",
    "WORDLINE_REGEX = re.compile(r'([\\d.]+)\\s(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)\\s(\\S+)\\s([\\d_]+)\\s(\\S+)\\s(\\S+)\\s(\\S+)')\n",
    "\n",
    "class Role:\n",
    "    def __init__(self, arg, start, end):\n",
    "        self.arg = arg\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def get_headline(self, sentence):\n",
    "        wordlines = sentence.wordlines\n",
    "        wordline = wordlines[self.start]\n",
    "        # go through wordlines within range\n",
    "        head = int(wordline.head) - 1\n",
    "        while head >= self.start and head <= self.end:\n",
    "            wordline = wordlines[head]\n",
    "            head = int(wordline.head) - 1\n",
    "        return wordline, int(wordline.sid)-1\n",
    "\n",
    "class Roleset:\n",
    "    def __init__(self, sid=None, roles=None, roleset_id=None):\n",
    "        self.sid = sid if sid is not None else ''\n",
    "        self.roles = roles if roles is not None else []\n",
    "        self.roleset_id = roleset_id if roleset_id is not None else ''\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, rolesets=None, wordlines=None):\n",
    "        self.rolesets = rolesets if rolesets is not None else []\n",
    "        self.wordlines = wordlines if wordlines is not None else []\n",
    "        \n",
    "class Skelline:\n",
    "    def __init__(self, index, roleset_id, args):\n",
    "        self.index = index\n",
    "        self.roleset_id = roleset_id\n",
    "        self.args = args\n",
    "        \n",
    "class Wordline:\n",
    "    def __init__(self, line):\n",
    "        matcher = wordline_regex.match(line)\n",
    "        if matcher:\n",
    "            self.sid = matcher.group(1)\n",
    "            self.form = matcher.group(2)\n",
    "            self.lemma = matcher.group(3)\n",
    "            self.upos = matcher.group(4)\n",
    "            self.xpos = matcher.group(5)\n",
    "            self.feats = matcher.group(6)\n",
    "            self.head = matcher.group(7)\n",
    "            self.deprel = matcher.group(8)\n",
    "            self.deps = matcher.group(9)\n",
    "            self.misc = matcher.group(10)\n",
    "    \n",
    "    def to_str(self):\n",
    "        return self.sid + \"\\t\" +self.form + \"\\t\" +self.lemma + \"\\t\" +self.upos + \"\\t\" +self.xpos + \"\\t\" +self.feats + \"\\t\" +self.head + \"\\t\" +self.deprel + \"\\t\" +self.deps + \"\\t\" +self.misc + \"\\n\"\n",
    "\n",
    "def count_parentheses(s):\n",
    "    return s.count(')') - s.count('(')\n",
    "\n",
    "def process_skel_line(matcher):\n",
    "    index = int(matcher.group(1))\n",
    "    roleset_id = matcher.group(2)\n",
    "    arg_str = matcher.group(3).strip()\n",
    "    args = SPACE_REGEX.split(arg_str) if len(arg_str) > 0 else []\n",
    "    return Skelline(index, roleset_id, args)\n",
    "\n",
    "# given a list of skel lines for a sentence, output a list of rolesets\n",
    "def get_rolesets_from_skel(skel_lines):\n",
    "    # a list of lists, where each list represents a column in the skel file\n",
    "    arg_list = []\n",
    "    # the rolesets in the skel file\n",
    "    rolesets = []\n",
    "    # number of columns\n",
    "    length = len(skel_lines[0].args)\n",
    "    # initialize the arglist and rolesets list\n",
    "    for i in range(length):\n",
    "        arg_list.append([])\n",
    "        rolesets.append(Roleset())\n",
    "    \n",
    "    # traverse each line\n",
    "    for skel_line in skel_lines:\n",
    "        # get the roleset id\n",
    "        roleset_id = skel_line.roleset_id\n",
    "        # go through each column\n",
    "        for j in range(length):\n",
    "            # set arg to current arg\n",
    "            arg = skel_line.args[j]\n",
    "            # if arg is rel and roleset_id is not null, set corresponding roleset_id\n",
    "            if REL_REGEX.match(arg) and roleset_id != '_':\n",
    "                rolesets[j].sid = roleset_id\n",
    "                rolesets[j].index = skel_line.index\n",
    "            # add this arg to arglist at list j\n",
    "            arg_list[j].append(skel_line.args[j])\n",
    "    for i in range(len(arg_list)):\n",
    "        # go through skelarg lists and determine ranges\n",
    "        arg_name = ''\n",
    "        count = 0\n",
    "        start = 0\n",
    "        end = 0\n",
    "        # step through items in column\n",
    "        l = arg_list[i]\n",
    "        for j in range(len(l)):\n",
    "            l_str = l[j]\n",
    "            matcher = ARG_REGEX.match(l_str)\n",
    "            if matcher:\n",
    "                arg_name = matcher.group(1)\n",
    "                start = j\n",
    "            count += count_parentheses(l_str)\n",
    "            if count == 0 and l_str != '*':\n",
    "                end = j\n",
    "                rolesets[i].roles.append(Role(arg_name, start, end))\n",
    "    return rolesets\n",
    "\n",
    "# reads in a .gold_skel file and returns a list of Sentence objects\n",
    "def get_sentences_from_skel(skel_path):\n",
    "    skel_lines = []\n",
    "    sentences = []\n",
    "    \n",
    "    for line in skel_path:\n",
    "        matcher = SKEL_REGEX.match(line)\n",
    "        # if matches\n",
    "        if matcher: skel_lines.append(process_skel_line(matcher))\n",
    "        # else, we've reached the end of a sentence\n",
    "        else:\n",
    "            sentence = Sentence()\n",
    "            sentence.rolesets = get_rolesets_from_skel(skel_lines[:])\n",
    "            sentences.append(sentence)\n",
    "            skel_lines = []\n",
    "    return sentences\n",
    "\n",
    "# adds wordlines from ud file to each sentence in sentences list\n",
    "def add_wordlines_from_ud_file(ud_path, sentences):\n",
    "    i = 0\n",
    "    # read ud file and add wordlines\n",
    "    for line in ud_path:\n",
    "        if line[0] != '#':\n",
    "            line_matcher = WORDLINE_REGEX.match(line)\n",
    "            # if line matches, add it to current sentence's wordline list\n",
    "            if line_matcher:\n",
    "                if '.' not in line_matcher.group(1): sentences[i].wordlines.append(Wordline(line))\n",
    "            # else, move onto the next sentence, if there is one\n",
    "            else:\n",
    "                i += 1\n",
    "                if i >= len(sentences): break\n",
    "    return sentences\n",
    "\n",
    "def create_annotated_sentence_string(sentence):\n",
    "    for roleset in sentence.rolesets:\n",
    "        for role in roleset.roles:\n",
    "            wordline, i = role.get_headline(sentence)\n",
    "            if role.arg is 'V':\n",
    "                sentence.wordlines[i].misc += '\\t' + roleset.sid\n",
    "                break\n",
    "    return sentence\n",
    "\n",
    "# creates the augmented ud file\n",
    "def create_output_file(file_list, sentences):\n",
    "    with open(file_list + '_ewt.txt', 'w') as f:\n",
    "        # get head for each role in sentence\n",
    "        for sentence in sentences:\n",
    "            sentence = create_annotated_sentence_string(sentence)\n",
    "            for wordline in sentence.wordlines:\n",
    "                w_str = wordline.to_str()\n",
    "                f.write(w_str)\n",
    "\n",
    "PROPBANK = 'data/propbank-release-master/data/google/ewt/'\n",
    "UD = 'data/UD_English-EWT/not-to-release/'\n",
    "FILE_LISTS = ['test']\n",
    "\n",
    "for file_list in FILE_LISTS:\n",
    "    file_name = UD + 'file-lists/files.' + file_list\n",
    "    with open(file_name, 'r') as f:\n",
    "        sentences = []\n",
    "    \n",
    "        for line in f:\n",
    "            # get .gold_skel file and .conllu file\n",
    "            skel_file = PROPBANK + line[:-7] + 'gold_skel'\n",
    "            ud_file = UD + 'sources/' + line.strip()\n",
    "            # if they both exist, process\n",
    "            if os.path.exists(skel_file) and os.path.exists(ud_file):\n",
    "                # get sentences from file\n",
    "                with open(skel_file, 'r') as skel_path: curr_sentences = get_sentences_from_skel(skel_path)\n",
    "                with open(ud_file, 'r') as ud_path:\n",
    "                    sentences += add_wordlines_from_ud_file(ud_path, curr_sentences)\n",
    "        create_output_file(file_list, sentences)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the following variables:\n",
    "- the filename prefixes of the training and dev data.\n",
    "- the NLTK WordNet lemmatizer.\n",
    "- the regex to capture each line in the annotation data text files. The annotation format, CoNLL-U, is described [here][1].\n",
    "\n",
    "Define a class to store the line data.\n",
    "\n",
    "[1]: http://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAMES = ['train_combine']\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "LINE_REGEX = re.compile(r'^(\\d+)\\t(\\S+)\\t\\S+\\t\\S+\\t(\\S+)\\t\\S+\\t(\\d+)\\t([a-zROT:]+)(?:\\S+)?\\t\\S+\\t\\S+(?:(?:\\t(\\S+))?)?')\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, index, form, pos, head, sr, roleset):\n",
    "        self.index = index\n",
    "        self.form = form\n",
    "        self.pos = pos\n",
    "        self.head = head\n",
    "        self.sr = sr\n",
    "        self.roleset = roleset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the train and dev data and find the sentences in which the defined lemma occurs. From there we can extract the desired features. [Mohammad and Pedersen (2004)][1] evaluate different lexical and syntactic features used in supervised word sense disambiguation. The features chosen here are inspired by this work but use [sn-grams][2] instead of n-grams.\n",
    "\n",
    "\n",
    "The features used here are:\n",
    "\n",
    "- the syntactic trigram centered on the target word.\n",
    "- the part-of-speech (POS) tags of each word in the trigram.\n",
    "- the syntactic relation (SR) tags of each word in the trigram.\n",
    "- the [WordNet lexicographer file name][3] for the two context words in the trigram.\n",
    "\n",
    "The label is the number associated with the roleset ID (e.g., the label associated with `be.01` is `'1'`).\n",
    "\n",
    "### Special cases\n",
    "#### Target words at the beginning or end of a sentence\n",
    "The trigram is truncated, and the POS and SR tags are assigned as `'START'` or `'END'`.\n",
    "#### Lemmatized token matches given lemma, but has no roleset\n",
    "The example is not used. (Alternatively, the roleset ID could be set to `'0'`.)\n",
    "\n",
    "#### Roleset ID is the non-numerical value `'LV'`\n",
    "The roleset ID is set to `n_rolesets + 1`.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Data paucity is a problem for many of the lemmata in the training data. [Zhang and LeCun (2015)][4] use a synonym replacement strategy to augment text data. Augment the training set by looking up the synonyms for the context words in each training example.\n",
    "\n",
    "[1]: http://www.aclweb.org/anthology/W04-2404\n",
    "[2]: http://www.cic.ipn.mx/~sidorov/Synt_n_grams_ESWA_FINAL.pdf\n",
    "[3]: https://wordnet.princeton.edu/documentation/lexnames5wn\n",
    "[4]: https://arxiv.org/pdf/1502.01710.pdf#page=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_token(lemma_name):\n",
    "    return '_' not in lemma_name and ' ' not in lemma_name\n",
    "\n",
    "def is_not_same_word(x, y):\n",
    "    return x.lower() != y.lower()\n",
    "\n",
    "def is_comparative(pos):\n",
    "    return pos[-1] == 'R'\n",
    "\n",
    "def is_superlative(pos):\n",
    "    return pos[-1] == 'S'\n",
    "\n",
    "def return_self(word):\n",
    "    return word\n",
    "\n",
    "def adverb_has_pertainym(synsets):\n",
    "    if len(synsets) > 0:\n",
    "        lemmas = synsets[0].lemmas()\n",
    "        if len(lemmas) > 0:\n",
    "            if len(lemmas[0].pertainyms()) > 0:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_syns(function, word, pos, wn_pos):\n",
    "    synonyms = set()\n",
    "    \n",
    "    lemmatized_word = LEMMATIZER.lemmatize(word, wn_pos)           \n",
    "    synsets = wn.synsets(lemmatized_word, wn_pos)\n",
    "    if wn_pos == wn.ADV:\n",
    "        if lemmatized_word == word and adverb_has_pertainym(synsets):\n",
    "            lemmatized_word = synsets[0].lemmas()[0].pertainyms()[0].name()\n",
    "            synsets = wn.synsets(lemmatized_word, wn.ADJ)\n",
    "        else:\n",
    "            return synonyms\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if wn_pos == wn.VERB:\n",
    "                synonyms.add((en.conjugate(lemma.name(), pos),synset.lexname()))\n",
    "            else:\n",
    "                synonyms.add((function(lemma.name()),synset.lexname()))\n",
    "    return synonyms\n",
    "\n",
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    is_adj = pos[0] == 'J'\n",
    "    is_common_noun = pos == 'NN' or pos == 'NNS'\n",
    "    is_adv = pos[:2] == 'RB'\n",
    "    is_verb = pos[0] == 'V'\n",
    "    function = return_self\n",
    "    wn_pos = wn.ADV\n",
    "    \n",
    "    if is_adj or is_adv:\n",
    "        if is_comparative(pos):\n",
    "            function = en.comparative\n",
    "        elif is_superlative(pos):\n",
    "            function = en.superlative\n",
    "        if is_adj:\n",
    "            wn_pos = wn.ADJ\n",
    "    elif is_common_noun:\n",
    "        if pos[-1] == 'S':\n",
    "            function = en.pluralize\n",
    "        else:\n",
    "            function = en.singularize\n",
    "        wn_pos = wn.NOUN\n",
    "    elif is_verb:\n",
    "        wn_pos = wn.VERB\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    synonyms = get_syns(function, word, pos, wn_pos)\n",
    "    return [x for x in synonyms if is_single_token(x[0]) and is_not_same_word(x[0], word)]\n",
    "\n",
    "def get_lexname(word, pos):\n",
    "    first_char = pos[0]\n",
    "    wn_pos = None\n",
    "    if first_char == 'J': wn_pos = wn.ADJ\n",
    "    elif first_char == 'N': wn_pos = wn.NOUN\n",
    "    elif first_char == 'V': wn_pos = wn.VERB\n",
    "    elif first_char == 'R': wn_pos = wn.ADV\n",
    "    else: return ' '\n",
    "    \n",
    "    lemmatized = lemmatizer.lemmatize(word, wn_pos)\n",
    "    synsets = wn.synsets(lemmatized, wn_pos)\n",
    "    if len(synsets) > 0:\n",
    "        return synsets[0].lexname()\n",
    "    return ' '\n",
    "\n",
    "def get_wn(token):\n",
    "    first_char = token.pos[0]\n",
    "    pos = ''\n",
    "    if first_char == 'J': pos = wn.ADJ\n",
    "    elif first_char == 'N': pos = wn.NOUN\n",
    "    elif first_char == 'V': pos = wn.VERB\n",
    "    elif first_char == 'R': pos = wn.ADV\n",
    "    else: return ' ', ' ', ' '\n",
    "    \n",
    "    synsets = wn.synsets(token.form, pos=pos)\n",
    "    if len(synsets) > 0:\n",
    "        defn = synsets[0].definition()\n",
    "        lexname = synsets[0].lexname()\n",
    "        lemmata = [' ']\n",
    "        for synset in synsets[0].hypernyms():\n",
    "            for lemma in synset.lemmas():\n",
    "                lemmata += lemma.key().split('%')[0].split('_')\n",
    "        return defn, ' '.join(set(lemmata)), lexname \n",
    "    else: return ' ', ' ', ' '\n",
    "\n",
    "def in_aliases(word_form, pos, lemma, rolesets):\n",
    "    lemmatized = lemmatizer.lemmatize(word_form.lower(), pos)\n",
    "    for alias in rolesets[0][0]:\n",
    "        if lemmatized == alias.text: return True\n",
    "    return False\n",
    "\n",
    "def create_files(lemma):\n",
    "    xml_rolesets = propbank.rolesets(lemma)\n",
    "    n_rolesets = len(xml_rolesets)\n",
    "    for filename in FILENAMES:\n",
    "        prev_pos = []\n",
    "        target_pos = []\n",
    "        next_pos = []\n",
    "\n",
    "        prev_sr = []\n",
    "        target_sr = []\n",
    "        next_sr = []\n",
    "\n",
    "        prev_defn = []\n",
    "        next_defn = []\n",
    "\n",
    "        prev_hypernym = []\n",
    "        next_hypernym = []\n",
    "\n",
    "        prev_lexname = []\n",
    "        next_lexname = []\n",
    "        \n",
    "        \n",
    "        #ngrams = []\n",
    "        prev_word = []\n",
    "        target_word = []\n",
    "        next_word = []\n",
    "        rolesets = []\n",
    "        lv = []\n",
    "        offset = 0\n",
    "\n",
    "        with open(filename + '.txt', 'r') as f:\n",
    "            line_num = 1\n",
    "            len_rolesets = 0\n",
    "            sentence = []\n",
    "            training_examples = []\n",
    "            for line in f:\n",
    "                result = line_regex.match(line)\n",
    "                if result:\n",
    "                    # get match groups\n",
    "                    index = int(result.group(1))\n",
    "                    form = result.group(2)\n",
    "                    pos = result.group(3)\n",
    "                    head = int(result.group(4))\n",
    "                    sr = result.group(5)\n",
    "                    roleset = result.group(6)\n",
    "                    # if new sentence, process old sentence and set beginning of new sentence\n",
    "                    if index < line_num:\n",
    "                        for i, word in enumerate(sentence):\n",
    "                            first_char = word.pos[0].lower()\n",
    "                            # TODO: adjectives\n",
    "                            if first_char in ['n', 'v'] and in_aliases(word.form, first_char, lemma, xml_rolesets) and word.roleset is not None and '.' in word.roleset:\n",
    "                                #if word.roleset == None:\n",
    "                                #    rolesets.append('0')\n",
    "                                #else:\n",
    "                                arr = word.roleset.split('.')\n",
    "                                num = word.roleset.split('.')[1]\n",
    "                                if num == 'LV':\n",
    "                                    rolesets.append(str(n_rolesets+1))\n",
    "                                    lv.append(len_rolesets-1)\n",
    "                                else:\n",
    "                                    num = int(num)\n",
    "                                    rolesets.append(str(num))\n",
    "                                    if num > n_rolesets:\n",
    "                                        n_rolesets = num\n",
    "                                len_rolesets += 1\n",
    "                                \n",
    "                                #form_list = [word.form]\n",
    "                                prev_form = '<<START>>'\n",
    "                                target_word.append(word.form)\n",
    "                                next_form = '<<END>>'\n",
    "                                # target word\n",
    "                                target_pos.append(word.pos)\n",
    "                                target_sr.append(word.sr.split(':')[0])\n",
    "\n",
    "                                #Create the sn-gram\n",
    "                                curr_i = i-1\n",
    "                                while curr_i >= 0:\n",
    "                                    #traverse backwards\n",
    "                                    #if token is head or child of curr\n",
    "                                    prev_tok = sentence[curr_i]\n",
    "                                    if prev_tok.index == word.head or prev_tok.head == word.index:\n",
    "                                        #form_list.insert(0, prev_tok.form)\n",
    "                                        prev_form = prev_tok.form\n",
    "                                        prev_pos.append(prev_tok.pos)\n",
    "                                        prev_sr.append(prev_tok.sr.split(':')[0])\n",
    "                                        p_defn, p_hypernym, p_lexname = get_wn(prev_tok)\n",
    "                                        prev_defn.append(p_defn)\n",
    "                                        prev_hypernym.append(p_hypernym)\n",
    "                                        prev_lexname.append(p_lexname)\n",
    "                                        break\n",
    "                                    else: curr_i -= 1\n",
    "                                if curr_i < 0:\n",
    "                                    prev_pos.append('START')\n",
    "                                    prev_sr.append('START')\n",
    "                                    prev_defn.append(' ')\n",
    "                                    prev_hypernym.append(' ')\n",
    "                                    prev_lexname.append('START')\n",
    "\n",
    "                                curr_i = i+1\n",
    "                                while curr_i < len(sentence):\n",
    "                                    #traverse forwards\n",
    "                                    #if token is head or child of curr\n",
    "                                    next_tok = sentence[curr_i]\n",
    "                                    if next_tok.index == word.head or next_tok.head == word.index:\n",
    "                                        #form_list.append(next_tok.form)\n",
    "                                        next_form = next_tok.form\n",
    "                                        next_pos.append(next_tok.pos)\n",
    "                                        next_sr.append(next_tok.sr.split(':')[0])\n",
    "                                        n_defn, n_hypernym, n_lexname = get_wn(next_tok)\n",
    "                                        next_defn.append(n_defn)\n",
    "                                        next_hypernym.append(n_hypernym)\n",
    "                                        next_lexname.append(n_lexname)\n",
    "                                        break\n",
    "                                    else: curr_i += 1\n",
    "                                if curr_i is len(sentence):\n",
    "                                    next_pos.append('END')\n",
    "                                    next_sr.append('END')\n",
    "                                    next_defn.append(' ')\n",
    "                                    next_hypernym.append(' ')\n",
    "                                    next_lexname.append('END')\n",
    "\n",
    "                                #ngrams.append(' '.join(form_list))\n",
    "                                prev_word.append(prev_form)\n",
    "                                next_word.append(next_form)\n",
    "                                \n",
    "                                #data augmentation\n",
    "                                #if filename == 'train_combine':\n",
    "                                #    for prev_paraphrase in get_paraphrases(prev_word[-1], prev_pos[-1]):\n",
    "                                #        for next_paraphrase in get_paraphrases(next_word[-1], next_pos[-1]):\n",
    "                                    #for prev_synonym in get_synonyms(prev_word[-1], prev_pos[-1]):\n",
    "                                    #    for next_synonym in get_synonyms(next_word[-1], next_pos[-1]):\n",
    "                                #            if rolesets[-1] == n_rolesets+1:\n",
    "                                #                lv.append(len_rolesets)\n",
    "                                #            rolesets.append(rolesets[-1])\n",
    "                                        \n",
    "                                #            prev_word.append(prev_paraphrase)\n",
    "                                #            target_word.append(target_word[-1])\n",
    "                                #            next_word.append(next_paraphrase)\n",
    "                                                                                         \n",
    "                                #            prev_pos.append(prev_pos[-1])\n",
    "                                #            target_pos.append(target_pos[-1])\n",
    "                                #            next_pos.append(next_pos[-1])\n",
    "\n",
    "                                #            prev_sr.append(prev_sr[-1])\n",
    "                                #            target_sr.append(target_sr[-1])\n",
    "                                #            next_sr.append(next_sr[-1])\n",
    "\n",
    "                                #            prev_defn.append(prev_defn[-1])\n",
    "                                #            next_defn.append(next_defn[-1])\n",
    "\n",
    "                                #            prev_hypernym.append(prev_hypernym[-1])\n",
    "                                #            next_hypernym.append(prev_hypernym[-1])\n",
    "\n",
    "                                #            prev_lexname.append(get_lexname(prev_paraphrase, prev_pos[-1]))\n",
    "                                #            next_lexname.append(get_lexname(next_paraphrase, next_pos[-1]))\n",
    "                                \n",
    "                        # reset sentence\n",
    "                        sentence = [Line(index, form, pos, head, sr, roleset)]\n",
    "                    # else, add to existing sentence\n",
    "                    else: sentence.append(Line(index, form, pos, head, sr, roleset))\n",
    "                    line_num = index\n",
    "\n",
    "        #add offset to LVs\n",
    "        for i in lv: rolesets[i] = str(n_rolesets+1)\n",
    "\n",
    "        with open('processed/' + lemma + '_' + filename + '_examples_unaugmented.txt', 'w') as examples:\n",
    "            for i in range(len(rolesets)):\n",
    "                examples.write('\\t'.join([prev_word[i], target_word[i], next_word[i], prev_pos[i], target_pos[i], next_pos[i], prev_sr[i], target_sr[i], next_sr[i], prev_defn[i], next_defn[i], prev_hypernym[i], next_hypernym[i], prev_lexname[i], next_lexname[i], rolesets[i]]) + '\\n')\n",
    "                #examples.write('\\t'.join([ngrams[i], prev_pos[i], target_pos[i], next_pos[i], prev_sr[i], target_sr[i], next_sr[i], prev_defn[i], next_defn[i], prev_hypernym[i], next_hypernym[i], prev_lexname[i], next_lexname[i], rolesets[i]]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier\n",
    "\n",
    "All code below this point is run outside the notebook, as a separate `classifier.py` Python file. This is because the events file [currently do not save correctly in a Jupyter notebook](https://stackoverflow.com/a/51232738). The classifier can be run on the command line:\n",
    "\n",
    "`python classifier.py`\n",
    "\n",
    "TensorFlow defines a class called an [Estimator][1], which is used to train and evaluate models. This model uses a premade Estimator called a [DNNClassifier][2]. Code derived from [this tutorial][3].\n",
    "\n",
    "Define a function to load data into a DataFrame.\n",
    "\n",
    "[1]: https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/Estimator\n",
    "[2]: https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/DNNClassifier\n",
    "[3]: https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    n_rolesets = 1\n",
    "    \n",
    "    data = {}\n",
    "    #data['ngram'] = []\n",
    "    data['prev_word'] = []\n",
    "    data['target_word'] = []\n",
    "    data['next_word'] = []\n",
    "    \n",
    "    data['prev_pos'] = []\n",
    "    data['target_pos'] = []\n",
    "    data['next_pos'] = []\n",
    "    \n",
    "    data['prev_sr'] = []\n",
    "    data['target_sr'] = []\n",
    "    data['next_sr'] = []\n",
    "    \n",
    "    #data['prev_defn'] = []\n",
    "    #data['next_defn'] = []\n",
    "    \n",
    "    #data['prev_hypernym'] = []\n",
    "    #data['next_hypernym'] = []\n",
    "    \n",
    "    data['prev_lexname'] = []\n",
    "    data['next_lexname'] = []\n",
    "    \n",
    "    data['roleset'] = []\n",
    "    with open(filename, 'r') as f:\n",
    "            prog = re.compile(r'^(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t(\\S+)\\t([\\S ]+)\\t([\\S ]+)\\t([\\S ]+)\\t([\\S ]+)\\t([\\S ]+)\\t([\\S ]+)\\t(\\d+)$')\n",
    "            for line in f:\n",
    "                result = prog.match(line)\n",
    "                if result:\n",
    "                    data['prev_word'].append(result.group(1))\n",
    "                    data['target_word'].append(result.group(2))\n",
    "                    data['next_word'].append(result.group(3))\n",
    "                    \n",
    "                    data['prev_pos'].append(result.group(4))\n",
    "                    data['target_pos'].append(result.group(5))\n",
    "                    data['next_pos'].append(result.group(6))\n",
    "                    data['prev_sr'].append(result.group(7))\n",
    "                    data['target_sr'].append(result.group(8))\n",
    "                    data['next_sr'].append(result.group(9))\n",
    "                    \n",
    "                    #data['prev_defn'].append(result.group(10))\n",
    "                    #data['next_defn'].append(result.group(11))\n",
    "                    \n",
    "                    #data['prev_hypernym'].append(result.group(12))\n",
    "                    #data['next_hypernym'].append(result.group(13))\n",
    "                    \n",
    "                    data['prev_lexname'].append(result.group(14))\n",
    "                    data['next_lexname'].append(result.group(15))\n",
    "                    \n",
    "                    roleset = int(result.group(16))\n",
    "                    if roleset > n_rolesets: n_rolesets = roleset\n",
    "                    \n",
    "                    data['roleset'].append(roleset)\n",
    "    return pd.DataFrame.from_dict(data), n_rolesets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the feature columns\n",
    "\n",
    "The model uses [feature columns][1] to transform the data into a format compatible with the Estimator.\n",
    "\n",
    "Define the possible labels for POS tags. This consists of the [Penn Treebank POS tags][2], plus the special `'START'` and `'END'` tags and some punctuation tags.\n",
    "\n",
    "Define the possible labels for the SR tags. This consists of the [Universal Dependencies relations][3], plus the special `'neg'`, `'START'`, `'END'`, and `'ROOT'` tags.\n",
    "\n",
    "[1]: https://www.tensorflow.org/guide/feature_columns\n",
    "[2]:https://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports#page=8\n",
    "[3]: http://universaldependencies.org/u/dep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['START', 'END', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'HYPH', '\\'\\'', '.', ':', ',', ')', '(', '``', '$' ]\n",
    "sr_list = ['START', 'END', 'ROOT', 'neg', 'acl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'case', 'cc', 'ccomp', 'clf', 'compound', 'conj', 'cop', 'csubj', 'dep', 'det', 'discourse', 'dislocated', 'expl', 'fixed', 'flat', 'goeswith', 'iobj', 'list', 'mark', 'nmod', 'nsubj', 'nummod', 'obj', 'obl', 'orphan', 'parataxis', 'punct', 'reparandum', 'root', 'vocative', 'xcomp']\n",
    "lexname_list = ['START', 'END', ' ', 'adj.all', 'adj.pert', 'adv.all', 'noun.Tops', 'noun.act', 'noun.animal', 'noun.artifact', 'noun.attribute', 'noun.body', 'noun.cognition', 'noun.communication', 'noun.event', 'noun.feeling', 'noun.food', 'noun.group', 'noun.location', 'noun.motive', 'noun.object', 'noun.person', 'noun.phenomenon', 'noun.plant', 'noun.possession', 'noun.process', 'noun.quantity', 'noun.relation', 'noun.shape', 'noun.state', 'noun.substance', 'noun.time', 'verb.body', 'verb.change', 'verb.cognition', 'verb.communication', 'verb.competition', 'verb.consumption', 'verb.contact', 'verb.creation', 'verb.emotion', 'verb.motion', 'verb.perception', 'verb.possession', 'verb.social', 'verb.stative', 'verb.weather', 'adj.ppl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test data into DataFrames.\n",
    "\n",
    "Represent the POS and SR tags in [categorical vocabulary columns](https://www.tensorflow.org/guide/feature_columns#categorical_vocabulary_column), which are essentially one-hot vectors. The model should learn some relationships between the categories, since some of them are related (e.g., adjectives, nouns, adverbs, verbs, etc. for POS; nominals, clauses, etc. for SR), so wrap the categorical columns in a lower-dimension [embedding column](https://www.tensorflow.org/guide/feature_columns#indicator_and_embedding_columns).\n",
    "\n",
    "[ExampleCheckpointSaverListener](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointSaverListener)\n",
    "\n",
    "Define a function to train and evaluate a model using a given [text module](https://www.tensorflow.org/hub/modules/text). These modules define text embeddings that can be used to represent our trigrams in a feature column.\n",
    "\n",
    "The model implements accuracy-based [early stopping](https://github.com/tensorflow/tensorflow/issues/18394)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleCheckpointSaverListener(tf.train.CheckpointSaverListener):\n",
    "    def __init__(self, estimator, dev_input_fn, best_accuracy):\n",
    "        self.estimator = estimator\n",
    "        self.dev_input_fn = dev_input_fn\n",
    "        self.best_accuracy = best_accuracy\n",
    "        \n",
    "    def begin(self):\n",
    "        # You can add ops to the graph here.\n",
    "        print('Starting the session.')\n",
    "\n",
    "    def before_save(self, session, global_step_value):\n",
    "        results = self.estimator.evaluate(input_fn=self.dev_input_fn)\n",
    "        print(results)\n",
    "        print('Best accuracy: ', self.best_accuracy)\n",
    "        if results['accuracy'] >= self.best_accuracy:\n",
    "            self.best_accuracy = results['accuracy']\n",
    "\n",
    "    def after_save(self, session, global_step_value):\n",
    "        print('Checkpoint saved')\n",
    "\n",
    "    def end(self, session, global_step_value):\n",
    "        print('Done with the session.')\n",
    "\n",
    "def create_shared_embedding_columns(keys, vocabulary_list, dimension):\n",
    "    column_list = []\n",
    "    for key in keys:\n",
    "        column_list.append(\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                key=key,\n",
    "                vocabulary_list=vocabulary_list))\n",
    "    return tf.feature_column.shared_embedding_columns(column_list, dimension=dimension)\n",
    "\n",
    "def train_and_evaluate_with_module(hub_module, lemma, train_module=False):\n",
    "    train_df, n_rolesets = load_data('processed/' + lemma + '_train_combine_examples_unaugmented.txt')\n",
    "    dev_df, dev_rolesets = load_data('processed/' + lemma + '_dev_combine_examples.txt')\n",
    "    test_df, test_rolesets = load_data('processed/' + lemma + '_test_combine_examples.txt')    \n",
    "\n",
    "    if dev_rolesets > n_rolesets: n_rolesets = dev_rolesets\n",
    "    if test_rolesets > n_rolesets: n_rolesets = test_rolesets    \n",
    "\n",
    "    # training input on whole training set with no limit on training epochs\n",
    "    train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df['roleset'], num_epochs=None, shuffle=True)\n",
    "\n",
    "    # dev input on whole training set with no limit on training epochs\n",
    "    dev_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    dev_df, dev_df['roleset'], num_epochs=None, shuffle=False)\n",
    "\n",
    "    # prediction on whole training set\n",
    "    predict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df['roleset'], batch_size=32, shuffle=True)\n",
    "\n",
    "    # training input on whole dev set\n",
    "    predict_dev_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    dev_df, dev_df['roleset'], batch_size=32, shuffle=False)\n",
    "\n",
    "    predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    test_df, test_df['roleset'], batch_size=32, shuffle=False)\n",
    "\n",
    "    #shared embeddiing columns\n",
    "    pos_feature_columns = create_shared_embedding_columns(['prev_pos', 'target_pos', 'next_pos'], pos_list, 5)\n",
    "    sr_feature_columns = create_shared_embedding_columns(['prev_sr', 'target_sr', 'next_sr'], sr_list, 12)\n",
    "    lexname_feature_columns = create_shared_embedding_columns(['prev_lexname', 'next_lexname'], lexname_list, 4)\n",
    "\n",
    "    text_feature_columns = [\n",
    "        hub.text_embedding_column(key='prev_word', module_spec=hub_module, trainable=train_module),\n",
    "        hub.text_embedding_column(key='target_word', module_spec=hub_module, trainable=train_module),\n",
    "        hub.text_embedding_column(key='next_word', module_spec=hub_module, trainable=train_module)]\n",
    "\n",
    "    model_dir = \"/home/tphan/Desktop/python/2018_summer/classifier/model/\" + lemma\n",
    "    feature_columns = text_feature_columns+pos_feature_columns+sr_feature_columns+lexname_feature_columns\n",
    "    #estimator = tf.estimator.BaselineClassifier(n_classes=n_rolesets+1,model_dir=model_dir)\n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        config=tf.estimator.RunConfig(keep_checkpoint_max=16),\n",
    "        hidden_units=[500,100],\n",
    "        feature_columns=text_feature_columns+pos_feature_columns+sr_feature_columns+lexname_feature_columns,\n",
    "        n_classes=n_rolesets+1,\n",
    "        optimizer=tf.train.AdamOptimizer(),\n",
    "        dropout=.5,\n",
    "        model_dir=model_dir)\n",
    "    os.makedirs(estimator.eval_dir())\n",
    "\n",
    "    best_accuracy = -1\n",
    "    listener = ExampleCheckpointSaverListener(estimator, predict_dev_input_fn, best_accuracy)\n",
    "    saver_hook = tf.train.CheckpointSaverHook(model_dir, listeners=[listener], save_steps=2000)\n",
    "    hook = tf.contrib.estimator.stop_if_no_decrease_hook(estimator, 'loss', 8000, min_steps=8000, run_every_secs=None, run_every_steps=2000)\n",
    "    estimator.train(input_fn=train_input_fn, hooks=[hook, saver_hook])\n",
    "    checkpoint_state_proto = tf.train.get_checkpoint_state(model_dir)\n",
    "    best_results = {}\n",
    "    best_predictions = []\n",
    "    best_checkpoint_path = None\n",
    "    test_results = {}\n",
    "    test_predictions = []\n",
    "    if checkpoint_state_proto is not None:\n",
    "        checkpoint_paths = checkpoint_state_proto.all_model_checkpoint_paths\n",
    "        if len(checkpoint_paths) > 0:\n",
    "            best_accuracy = 0\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                results = estimator.evaluate(input_fn=predict_dev_input_fn, checkpoint_path=checkpoint_path)\n",
    "                if results['accuracy'] >= best_accuracy:\n",
    "                    best_checkpoint_path = checkpoint_path\n",
    "                    best_accuracy = results['accuracy']\n",
    "                    best_results = results\n",
    "\n",
    "            for pred_dict in estimator.predict(predict_dev_input_fn, checkpoint_path=best_checkpoint_path):\n",
    "                best_predictions.append(pred_dict)\n",
    "            for pred_dict in estimator.predict(predict_test_input_fn, checkpoint_path=best_checkpoint_path):\n",
    "                test_predictions.append(pred_dict)\n",
    "            \n",
    "            test_results = estimator.evaluate(input_fn=predict_test_input_fn, checkpoint_path=best_checkpoint_path)\n",
    "            print('Results')\n",
    "            print(best_results)\n",
    "            print('Test results')\n",
    "            print(test_results)\n",
    "            #estimator.export_savedmodel('/home/tphan/Desktop/python/2018_summer/classifier/savedmodel/' + lemma,\n",
    "            #                            tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "            #                                tf.feature_column.make_parse_example_spec(feature_columns),\n",
    "            #                                32),\n",
    "            #                            checkpoint_path=checkpoint_paths[0],\n",
    "            #                            strip_default_attrs=True)\n",
    "            shutil.rmtree(model_dir)\n",
    "            return best_predictions, test_predictions, {\n",
    "                'Rolesets': str(int(n_rolesets)),\n",
    "                'Rolesets in dev': str(int(dev_rolesets)),\n",
    "                'Train size': str(int(train_df.shape[0])),\n",
    "                'Dev size': str(int(dev_df.shape[0])),\n",
    "                'Dev accuracy': best_results['accuracy'],\n",
    "                'Average loss': best_results['average_loss'],\n",
    "                'Loss': best_results['loss'],\n",
    "                'Global step': best_results['global_step']\n",
    "            }, {\n",
    "                'Rolesets': str(int(n_rolesets)),\n",
    "                'Rolesets in test': str(int(test_rolesets)),\n",
    "                'Train size': str(int(train_df.shape[0])),\n",
    "                'Test size': str(int(test_df.shape[0])),\n",
    "                'Test accuracy': test_results['accuracy'],\n",
    "                'Average loss': test_results['average_loss'],\n",
    "                'Loss': test_results['loss'],\n",
    "                'Global step': test_results['global_step']\n",
    "            }\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/tphan/Desktop/python/2018_summer/classifier/model/be', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f60651dc438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt.\n",
      "INFO:tensorflow:loss = 187.86261, step = 1\n",
      "INFO:tensorflow:global_step/sec: 133.924\n",
      "INFO:tensorflow:loss = 3.6758623, step = 101 (0.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.928\n",
      "INFO:tensorflow:loss = 12.352717, step = 201 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.388\n",
      "INFO:tensorflow:loss = 11.558242, step = 301 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.561\n",
      "INFO:tensorflow:loss = 2.319933, step = 401 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.484\n",
      "INFO:tensorflow:loss = 8.0559025, step = 501 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.874\n",
      "INFO:tensorflow:loss = 3.1391997, step = 601 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.782\n",
      "INFO:tensorflow:loss = 9.63292, step = 701 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.143\n",
      "INFO:tensorflow:loss = 3.6467142, step = 801 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.291\n",
      "INFO:tensorflow:loss = 4.288163, step = 901 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.045\n",
      "INFO:tensorflow:loss = 6.244405, step = 1001 (0.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.01\n",
      "INFO:tensorflow:loss = 1.186358, step = 1101 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.52\n",
      "INFO:tensorflow:loss = 2.2153661, step = 1201 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.836\n",
      "INFO:tensorflow:loss = 3.3488035, step = 1301 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.809\n",
      "INFO:tensorflow:loss = 0.9395658, step = 1401 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.643\n",
      "INFO:tensorflow:loss = 1.2004008, step = 1501 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.923\n",
      "INFO:tensorflow:loss = 8.764565, step = 1601 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.4\n",
      "INFO:tensorflow:loss = 1.9361651, step = 1701 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.435\n",
      "INFO:tensorflow:loss = 2.2982068, step = 1801 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.593\n",
      "INFO:tensorflow:loss = 0.7596696, step = 1901 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.921\n",
      "INFO:tensorflow:loss = 13.588171, step = 2001 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.962\n",
      "INFO:tensorflow:loss = 0.5687354, step = 2101 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.193\n",
      "INFO:tensorflow:loss = 9.301358, step = 2201 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.85\n",
      "INFO:tensorflow:loss = 1.3317974, step = 2301 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.138\n",
      "INFO:tensorflow:loss = 1.6142238, step = 2401 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.03\n",
      "INFO:tensorflow:loss = 1.9476035, step = 2501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.492\n",
      "INFO:tensorflow:loss = 3.8607879, step = 2601 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.107\n",
      "INFO:tensorflow:loss = 1.7333548, step = 2701 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.655\n",
      "INFO:tensorflow:loss = 7.4062514, step = 2801 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.944\n",
      "INFO:tensorflow:loss = 1.8342389, step = 2901 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.94\n",
      "INFO:tensorflow:loss = 0.7587957, step = 3001 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.818\n",
      "INFO:tensorflow:loss = 4.3597803, step = 3101 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.218\n",
      "INFO:tensorflow:loss = 8.267738, step = 3201 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.122\n",
      "INFO:tensorflow:loss = 4.9389877, step = 3301 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.733\n",
      "INFO:tensorflow:loss = 3.1603942, step = 3401 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.622\n",
      "INFO:tensorflow:loss = 3.5701597, step = 3501 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.885\n",
      "INFO:tensorflow:loss = 1.4413891, step = 3601 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.604\n",
      "INFO:tensorflow:loss = 2.0171785, step = 3701 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.547\n",
      "INFO:tensorflow:loss = 2.0257232, step = 3801 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.409\n",
      "INFO:tensorflow:loss = 0.1674414, step = 3901 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.719\n",
      "INFO:tensorflow:loss = 2.530047, step = 4001 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.984\n",
      "INFO:tensorflow:loss = 0.32548618, step = 4101 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.334\n",
      "INFO:tensorflow:loss = 1.7172942, step = 4201 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.616\n",
      "INFO:tensorflow:loss = 0.76551664, step = 4301 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.998\n",
      "INFO:tensorflow:loss = 0.7727523, step = 4401 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.784\n",
      "INFO:tensorflow:loss = 1.7984471, step = 4501 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.477\n",
      "INFO:tensorflow:loss = 0.96141446, step = 4601 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.513\n",
      "INFO:tensorflow:loss = 1.2492502, step = 4701 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.602\n",
      "INFO:tensorflow:loss = 1.2353323, step = 4801 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.548\n",
      "INFO:tensorflow:loss = 1.8857145, step = 4901 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.124\n",
      "INFO:tensorflow:loss = 0.3287244, step = 5001 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.951\n",
      "INFO:tensorflow:loss = 0.28097802, step = 5101 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.841\n",
      "INFO:tensorflow:loss = 2.0383098, step = 5201 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.189\n",
      "INFO:tensorflow:loss = 0.44480968, step = 5301 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.746\n",
      "INFO:tensorflow:loss = 1.3747587, step = 5401 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.044\n",
      "INFO:tensorflow:loss = 1.7719101, step = 5501 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.729\n",
      "INFO:tensorflow:loss = 10.153898, step = 5601 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.963\n",
      "INFO:tensorflow:loss = 5.727518, step = 5701 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.608\n",
      "INFO:tensorflow:loss = 2.0013032, step = 5801 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.148\n",
      "INFO:tensorflow:loss = 2.6302648, step = 5901 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.621\n",
      "INFO:tensorflow:loss = 2.1475363, step = 6001 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.459\n",
      "INFO:tensorflow:loss = 4.95072, step = 6101 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.759\n",
      "INFO:tensorflow:loss = 0.40820685, step = 6201 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.611\n",
      "INFO:tensorflow:loss = 0.051348276, step = 6301 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.346\n",
      "INFO:tensorflow:loss = 0.4859215, step = 6401 (0.517 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 195.937\n",
      "INFO:tensorflow:loss = 1.2495754, step = 6501 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.044\n",
      "INFO:tensorflow:loss = 0.5080174, step = 6601 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.383\n",
      "INFO:tensorflow:loss = 1.366303, step = 6701 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.295\n",
      "INFO:tensorflow:loss = 0.17961384, step = 6801 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.72\n",
      "INFO:tensorflow:loss = 2.46152, step = 6901 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.938\n",
      "INFO:tensorflow:loss = 0.5143526, step = 7001 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.761\n",
      "INFO:tensorflow:loss = 0.6283549, step = 7101 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.239\n",
      "INFO:tensorflow:loss = 3.4866638, step = 7201 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.715\n",
      "INFO:tensorflow:loss = 0.36126232, step = 7301 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.954\n",
      "INFO:tensorflow:loss = 1.1012791, step = 7401 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.903\n",
      "INFO:tensorflow:loss = 1.181702, step = 7501 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.439\n",
      "INFO:tensorflow:loss = 10.002712, step = 7601 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.295\n",
      "INFO:tensorflow:loss = 2.9001036, step = 7701 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.604\n",
      "INFO:tensorflow:loss = 0.054619513, step = 7801 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.991\n",
      "INFO:tensorflow:loss = 1.4127153, step = 7901 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.247\n",
      "INFO:tensorflow:loss = 0.21476796, step = 8001 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.663\n",
      "INFO:tensorflow:loss = 1.3310139, step = 8101 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.626\n",
      "INFO:tensorflow:loss = 0.17446835, step = 8201 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.722\n",
      "INFO:tensorflow:loss = 2.09061, step = 8301 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.028\n",
      "INFO:tensorflow:loss = 2.1191416, step = 8401 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.105\n",
      "INFO:tensorflow:loss = 2.5683494, step = 8501 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.407\n",
      "INFO:tensorflow:loss = 0.13031411, step = 8601 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.614\n",
      "INFO:tensorflow:loss = 0.26263827, step = 8701 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.793\n",
      "INFO:tensorflow:loss = 0.07744384, step = 8801 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.495\n",
      "INFO:tensorflow:loss = 1.9568651, step = 8901 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.3\n",
      "INFO:tensorflow:loss = 0.017492441, step = 9001 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.909\n",
      "INFO:tensorflow:loss = 0.29658872, step = 9101 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.501\n",
      "INFO:tensorflow:loss = 1.3962109, step = 9201 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.916\n",
      "INFO:tensorflow:loss = 0.87061155, step = 9301 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.377\n",
      "INFO:tensorflow:loss = 4.811531, step = 9401 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.702\n",
      "INFO:tensorflow:loss = 2.1658664, step = 9501 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.305\n",
      "INFO:tensorflow:loss = 0.10097736, step = 9601 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.891\n",
      "INFO:tensorflow:loss = 0.45383435, step = 9701 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.684\n",
      "INFO:tensorflow:loss = 6.1907415, step = 9801 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.68\n",
      "INFO:tensorflow:loss = 6.3728514, step = 9901 (0.511 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-03-18:47:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-03-18:47:06\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.9912005, average_loss = 0.08424444, global_step = 10000, loss = 2.6806579\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt-10000\n",
      "INFO:tensorflow:Loss for final step: 1.7037541.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: /home/tphan/Desktop/python/2018_summer/classifier/savedmodel/be/temp-b'1533322026'/assets\n",
      "INFO:tensorflow:SavedModel written to: /home/tphan/Desktop/python/2018_summer/classifier/savedmodel/be/temp-b'1533322026'/saved_model.pb\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/tphan/Desktop/python/2018_summer/classifier/model/be/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "lemmata = ['be', 'have', 'say', 'do', 'get', 'go', 'make', 'use', 'take', 'know', 'see', 'come', 'call', 'work', 'add', 'find', 'help', 'pay', 'try', 'look', 'operate', 'hold', 'tell', 'continue', 'serve', 'show', 'become', 'end', 'move', 'fall', 'concern', 'put', 'ask', 'keep', 'leave', 'change', 'run', 'spend', 'raise', 'lead', 'start', 'meet', 'feel', 'consider', 'send', 'develop', 'charge', 'build', 'lose', 'close', 'allow', 'grow', 'talk', 'mean', 'return', 'order', 'decline', 'cut', 'note', 'file', 'name', 'deal', 'base', 'act', 'live', 'appear', 'reach', 'view', 'follow', 'open', 'drop', 'manage', 'play', 'set', 'succeed', 'stop', 'happen', 'vote', 'turn', 'yield', 'stay', 'improve', 'question', 'pass', 'finance', 'drive', 'force', 'cover', 'stand', 'settle', 'argue', 'claim', 'fly', 'apply', 'process', 'review', 'rule', 'assume', 'love', 'introduce', 'resign', 'hit', 'affect', 'push', 'join', 'bid', 'sign', 'head', 'watch', 'treat', 'enter', 'care', 'gain', 'confer', 'refer', 'back', 'strike', 'walk', 'prepare', 'form', 'worry', 'save', 'break', 'bear', 'cite', 'compete', 'admit', 'encourage', 'aim', 'list', 'perform', 'measure', 'conclude', 'hurt', 'fill', 'ease', 'retire', 'contend', 'recover', 'realize', 'sound', 'slow', 'jump', 'draw', 'trip', 'fix', 'conduct', 'execute', 'miss', 'identify', 'recall', 'pull', 'promote', 'approach', 'tend', 'resolve', 'discount', 'throw', 'finish', 'extend', 'clear', 'matter', 'check', 'address', 'notice', 'employ', 'appreciate', 'suppose', 'restore', 'express', 'train', 'recognize', 'mark', 'commit', 'emerge', 'stem', 'feed', 'catch', 'spread', 'shoot', 'concentrate', 'climb', 'rally', 'count', 'soar', 'differ', 'cap', 'abandon', 'register', 'prompt', 'beat', 'plunge', 'track', 'plead', 'figure', 'warm', 'submit', 'press', 'fit', 'slip', 'project', 'exercise', 'divide', 'afford', 'double', 'arrest', 'seize', 'trust', 'insure', 'fire', 'sense', 'mount', 'mind', 'sustain', 'protest', 'lift', 'split', 'pick', 'locate', 'drink', 'contract', 'wave', 'struggle', 'crash', 'appeal', 'satisfy', 'pose', 'dismiss', 'cast', 'time', 'tie', 'cross', 'secure', 'assert', 'point', 'lease', 'land', 'squeeze', 'bother', 'slide', 'sleep', 'observe', 'halt', 'abuse', 'taste', 'swing', 'scare', 'relieve', 'explode', 'depress', 'credit', 'blow', 'bill', 'smoke', 'slash', 'rent', 'manipulate', 'illustrate', 'hang', 'evolve', 'contest', 'amount', 'weigh', 'prevail', 'trim', 'impress', 'dance', 'cheat', 'scramble', 'march', 'laugh', 'ring', 'burst', 'terminate', 'race', 'smell', 'paint', 'jolt', 'incorporate', 'excuse', 'cook', 'celebrate', 'tremor', 'surface', 'rest', 'top', 'tap', 'shed', 'leap', 'crack', 'assemble', 'prescribe', 'knock', 'compose', 'capitalize', 'unload', 'tape', 'scrap', 'retreat', 'freeze', 'upgrade', 'sweep', 'subscribe', 'motivate', 'fold', 'pitch', 'mistake', 'erupt', 'crowd', 'spell', 'render', 'dispose', 'correspond', 'swear', 'spin', 'seat', 'overlook', 'filter', 'dip', 'dictate', 'condition', 'classify', 'tip', 'sway', 'strain', 'screen', 'reckon', 'entitle', 'compromise', 'boom', 'bind', 'bend', 'upset', 'tear', 'sniff', 'scuttle', 'scale', 'hail', 'flash', 'curse', 'cry', 'command', 'bond', 'wrestle', 'stir', 'refinance', 'cheer', 'bleed', 'blast', 'venture', 'spare', 'pop', 'insulate', 'grind', 'galvanize', 'frame', 'flock', 'finger', 'divorce', 'code', 'circle', 'bow', 'balloon', 'stamp', 'snap', 'scratch', 'restate', 'reassert', 'rattle', 'plug', 'pile', 'pave', 'discharge', 'dawn', 'choke', 'bust', 'smash', 'lodge', 'heave', 'drool', 'cruise', 'conceive', 'bundle', 'appraise', 'wring', 'wiggle', 'weave', 'spurt', 'slam', 'skirt', 'skid', 'screech', 'ply', 'hook', 'fume', 'dispense', 'delight', 'blunder', 'accord', 'unhinge', 'stunt', 'repaint', 'recess', 'pinch', 'overbid', 'marvel', 'level', 'inaugurate', 'buzz']\n",
    "zero_dev = ['accord', 'amount', 'appraise', 'assemble', 'assert', 'balloon', 'bend', 'bind', 'bleed', 'blunder', 'boom', 'bow', 'bundle', 'bust', 'capitalize', 'cheat', 'choke', 'circle', 'classify', 'clear', 'code', 'command', 'condition', 'correspond', 'crack', 'cruise', 'curse', 'dawn', 'dictate', 'dip', 'discharge', 'dispense', 'divorce', 'drool', 'employ', 'entitle', 'erupt', 'evolve', 'exercise', 'figure', 'filter', 'finger', 'flash', 'fold', 'frame', 'fume', 'galvanize', 'grind', 'heave', 'hook', 'illustrate', 'impress', 'inaugurate', 'insure', 'jolt', 'laugh', 'leap', 'lease', 'lift', 'lodge', 'manipulate', 'measure', 'motivate', 'overbid', 'overlook', 'plead', 'pop', 'prescribe', 'project', 'race', 'reassert', 'recall', 'recess', 'refinance', 'register', 'render', 'repaint', 'restate', 'rest', 'restore', 'ring', 'scale', 'scare', 'scrap', 'scratch', 'screech', 'screen', 'seize', 'shed', 'skid', 'skirt', 'slam', 'slash', 'smash', 'snap', 'sniff', 'spare', 'spell', 'split', 'spurt', 'squeeze', 'stamp', 'stem', 'strain', 'surface', 'sway', 'swear', 'terminate', 'top', 'tremor', 'trim', 'unhinge', 'upset', 'warm', 'weave', 'wiggle', 'wrestle', 'bond', 'burst', 'buzz', 'celebrate', 'cheer', 'compose', 'cross', 'crowd', 'cry', 'delight', 'dispose', 'excuse', 'feed', 'halt', 'insulate', 'knock', 'level', 'march', 'marvel', 'pitch', 'plug', 'ply', 'reckon', 'resolve', 'retire', 'scramble', 'scuttle', 'seat', 'secure', 'slide', 'smoke', 'soar', 'spin', 'submit', 'tape', 'tap', 'tear', 'tip', 'unload', 'venture', 'weigh']\n",
    "lemmata = [x for x in lemmata if x not in zero_dev]\n",
    "\n",
    "results = {}\n",
    "for lemma in lemmata:\n",
    "    #create_files(lemma)\n",
    "    results[lemma] = train_and_evaluate_with_module('modules/1', lemma)\n",
    "    shutil.rmtree(\"/home/tphan/Desktop/python/2018_summer/classifier/model/\" + lemma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
